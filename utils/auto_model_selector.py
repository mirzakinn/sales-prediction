"""
Otomatik En Ä°yi Model SeÃ§ici
TÃ¼m algoritmalarÄ± test ederek en iyi performansÄ± veren modeli ve parametrelerini bulur.
"""

from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import lightgbm as lgb
import numpy as np
import pandas as pd
import time
import sys
import io
from concurrent.futures import ThreadPoolExecutor, TimeoutError
import signal


def auto_select_best_model(x_train, x_test, y_train, y_test, feature_columns=None, timeout_seconds=300):
    """
    TÃ¼m mevcut algoritmalarÄ± test ederek en iyi performansÄ± veren modeli bulur.
    
    Args:
        x_train: EÄŸitim verisi Ã¶zellikleri
        y_train: EÄŸitim verisi hedef deÄŸiÅŸkeni
        x_test: Test verisi Ã¶zellikleri
        y_test: Test verisi hedef deÄŸiÅŸkeni
        cv_folds: Cross-validation fold sayÄ±sÄ±
        detailed_mode: DetaylÄ± mod (daha fazla parametre kombinasyonu)
        max_time_per_model: Her model iÃ§in maksimum sÃ¼re (saniye)
    
    Returns:
        dict: En iyi model bilgileri
    """
    
    # Dataset boyutunu kontrol et
    n_samples = len(x_train)
    n_features = x_train.shape[1] if hasattr(x_train, 'shape') else len(x_train[0])
    
    # BÃ¼yÃ¼k dataset kategorilendirmesi - daha agresif
    is_large_dataset = n_samples > 30000 or n_features > 15
    is_huge_dataset = n_samples > 100000 or n_features > 25
    is_massive_dataset = n_samples > 300000
    
    if is_massive_dataset:
        detailed_mode = False
        max_time_per_model = 120
        cv_folds = 2
    elif is_huge_dataset:
        detailed_mode = False
        max_time_per_model = 180
        cv_folds = 3
    elif is_large_dataset:
        detailed_mode = False
        cv_folds = 3
    else:
        detailed_mode = True
        cv_folds = 5
    
    # Sampling fonksiyonu
    def smart_sample(x_train, y_train, x_test, y_test, sample_size=80000):
        from sklearn.model_selection import train_test_split
        
        if len(x_train) > sample_size:
            x_sample, _, y_sample, _ = train_test_split(
                x_train, y_train, train_size=sample_size, random_state=42
            )
            test_ratio = min(0.3, 20000 / len(x_test))
            x_test_sample, _, y_test_sample, _ = train_test_split(
                x_test, y_test, train_size=test_ratio, random_state=42
            )
            return x_sample, y_sample, x_test_sample, y_test_sample
        return x_train, y_train, x_test, y_test
    
    # Sampling uygula
    if is_huge_dataset or is_massive_dataset:
        x_train_work, y_train_work, x_test_work, y_test_work = smart_sample(
            x_train, y_train, x_test, y_test, sample_size=80000
        )
    else:
        x_train_work, y_train_work, x_test_work, y_test_work = x_train, y_train, x_test, y_test
    
    # ULTRA HIZLI mod parametre gridleri (massive dataset iÃ§in)
    ultra_minimal_params = {
        'linear_regression': {'fit_intercept': [True]},
        'ridge': {'alpha': [1.0]},
        'lasso': {'alpha': [1.0]},
        'elasticnet': {'alpha': [1.0], 'l1_ratio': [0.5]},
        'knn': {'n_neighbors': [5]},
        'svr': {'C': [1.0], 'gamma': ['scale']},
        'decision_tree': {'max_depth': [10]},
        'random_forest': {'n_estimators': [50], 'max_depth': [10]},
        'xgboost': {'n_estimators': [50], 'learning_rate': [0.1], 'max_depth': [6]},
        'lightgbm': {'n_estimators': [50], 'learning_rate': [0.1], 'max_depth': [10]}
    }
    
    # Ultra hÄ±zlÄ± mod parametre gridleri (bÃ¼yÃ¼k dataset iÃ§in)
    ultra_fast_params = {
        'linear_regression': {'fit_intercept': [True, False]},
        'ridge': {'alpha': [1.0, 10.0]},
        'lasso': {'alpha': [1.0, 10.0]},
        'elasticnet': {'alpha': [1.0], 'l1_ratio': [0.5, 0.7]},
        'knn': {'n_neighbors': [5, 10]},
        'svr': {'C': [1.0], 'gamma': ['scale']},
        'decision_tree': {'max_depth': [10, 15]},
        'random_forest': {'n_estimators': [50, 100], 'max_depth': [10]},
        'xgboost': {'n_estimators': [50, 100], 'learning_rate': [0.1], 'max_depth': [6]},
        'lightgbm': {'n_estimators': [50, 100], 'learning_rate': [0.1], 'max_depth': [10]}
    }
    
    # HÄ±zlÄ± mod parametre gridleri
    fast_params = {
        'linear_regression': {
            'fit_intercept': [True, False],
            'positive': [True, False]
        },
        'ridge': {
            'alpha': [0.1, 1.0, 10.0, 100.0],
            'solver': ['auto', 'svd', 'cholesky']
        },
        'lasso': {
            'alpha': [0.1, 1.0, 10.0, 100.0],
            'max_iter': [1000, 2000, 5000]
        },
        'elasticnet': {
            'alpha': [0.1, 1.0, 10.0],
            'l1_ratio': [0.1, 0.5, 0.7, 0.9]
        },
        'knn': {
            'n_neighbors': [3, 5, 7, 10],
            'weights': ['uniform', 'distance']
        },
        'svr': {
            'C': [0.1, 1.0, 10.0],
            'gamma': ['scale', 'auto']
        },
        'decision_tree': {
            'max_depth': [None, 5, 10, 15],
            'min_samples_leaf': [1, 2, 4, 8]
        },
        'random_forest': {
            'n_estimators': [50, 100, 200],
            'max_depth': [None, 5, 10, 15]
        },
        'xgboost': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 6, 9]
        },
        'lightgbm': {
            'n_estimators': [50, 100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [-1, 5, 10, 15]
        }
    }
    
    # DetaylÄ± mod parametre gridleri (daha kapsamlÄ±)
    detailed_params = {
        'linear_regression': {
            'fit_intercept': [True, False],
            'positive': [True, False]
        },
        'ridge': {
            'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0],
            'solver': ['auto', 'svd', 'cholesky', 'lsqr']
        },
        'lasso': {
            'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0],
            'max_iter': [1000, 2000, 3000, 5000, 8000]
        },
        'elasticnet': {
            'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],
            'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 0.95]
        },
        'knn': {
            'n_neighbors': [3, 5, 7, 9, 11, 15, 20],
            'weights': ['uniform', 'distance'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree']
        },
        'svr': {
            'C': [0.1, 0.5, 1.0, 5.0, 10.0, 50.0, 100.0],
            'gamma': ['scale', 'auto'],
            'kernel': ['rbf', 'linear', 'poly']
        },
        'decision_tree': {
            'max_depth': [None, 3, 5, 7, 10, 15, 20],
            'min_samples_leaf': [1, 2, 4, 6, 8, 12],
            'min_samples_split': [2, 5, 10, 15]
        },
        'random_forest': {
            'n_estimators': [50, 100, 150, 200, 300, 400],
            'max_depth': [None, 5, 10, 15, 20, 25],
            'min_samples_leaf': [1, 2, 4, 6]
        },
        'xgboost': {
            'n_estimators': [50, 100, 150, 200, 300, 400],
            'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3],
            'max_depth': [3, 4, 5, 6, 7, 8, 9],
            'subsample': [0.8, 0.9, 1.0]
        },
        'lightgbm': {
            'n_estimators': [50, 100, 150, 200, 300, 400],
            'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.3],
            'max_depth': [-1, 5, 7, 10, 15, 20],
            'num_leaves': [31, 50, 100, 150]
        }
    }
    
    # Parametre gridini seÃ§ - daha agresif
    if is_massive_dataset:
        param_grids = ultra_minimal_params
        cv_folds = 2
        mode_text = "ðŸ”¥ ULTRA MÄ°NÄ°MAL MOD (300K+ Dataset)"
    elif is_huge_dataset:
        param_grids = ultra_fast_params
        cv_folds = 3
        mode_text = "âš¡ ULTRA HIZLI MOD (100K+ Dataset)"
    elif is_large_dataset:
        param_grids = ultra_fast_params
        cv_folds = 3
        mode_text = "ðŸš€ HIZLI MOD (30K+ Dataset)"
    else:
        param_grids = detailed_params if detailed_mode else fast_params
        mode_text = "ðŸ“Š DETAYLI MOD" if detailed_mode else "âš¡ STANDART MOD"
    
    # TÃ¼m modeller ve seÃ§ilen parametre gridleri
    models_config = {
        'linear_regression': {
            'model': LinearRegression(),
            'params': param_grids['linear_regression']
        },
        'ridge': {
            'model': Ridge(),
            'params': param_grids['ridge']
        },
        'lasso': {
            'model': Lasso(),
            'params': param_grids['lasso']
        },
        'elasticnet': {
            'model': ElasticNet(),
            'params': param_grids['elasticnet']
        },
        'knn': {
            'model': KNeighborsRegressor(),
            'params': param_grids['knn']
        },
        'svr': {
            'model': SVR(),
            'params': param_grids['svr']
        },
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': param_grids['decision_tree']
        },
        'random_forest': {
            'model': RandomForestRegressor(),
            'params': param_grids['random_forest']
        },
        'xgboost': {
            'model': xgb.XGBRegressor(),
            'params': param_grids['xgboost']
        },
        'lightgbm': {
            'model': lgb.LGBMRegressor(verbose=-1),
            'params': param_grids['lightgbm']
        }
    }
    
    # AkÄ±llÄ± model sÄ±ralamasÄ± - hÄ±zlÄ± modeller Ã¶nce
    model_order = [
        'linear_regression',  # En hÄ±zlÄ±
        'ridge', 'lasso',     # HÄ±zlÄ± linear modeller
        'decision_tree',      # Orta hÄ±zlÄ±
        'elasticnet',
        'random_forest',      # Ensemble - orta
        'lightgbm',          # HÄ±zlÄ± gradient boosting
        'xgboost',           # YavaÅŸ gradient boosting
        'knn',               # YavaÅŸ (bÃ¼yÃ¼k data iÃ§in)
        'svr'                # En yavaÅŸ
    ]
    
    # Erken durma iÃ§in baseline belirleme
    baseline_r2 = -float('inf')
    early_stop_threshold = 0.1  # R2 < 0.1 ise modeli atla
    models_tested = 0
    max_models_to_test = 6 if is_massive_dataset else 8 if is_huge_dataset else 10
    
    results = []
    total_start_time = time.time()
    
    
    def train_single_model(model_name, config):
        """Tek model eÄŸitimi - timeout ile"""
        try:
            start_time = time.time()
            
            # GridSearchCV ile en iyi parametreleri bul
            grid_search = GridSearchCV(
                config['model'],
                config['params'],
                cv=cv_folds,
                scoring='r2',
                n_jobs=-1,
                verbose=0
            )
            
            # LGBMRegressor iÃ§in DataFrame kullan
            if model_name == 'lightgbm':
                # DataFrame'e Ã§evir
                x_train_work_df = pd.DataFrame(x_train_work, columns=feature_columns)
                y_train_work_df = pd.Series(y_train_work)
                
                grid_search.fit(x_train_work_df, y_train_work_df)
                
                # En iyi modeli test et
                best_model = grid_search.best_estimator_
                
                # BÃ¼yÃ¼k dataset ise final modeli tam veri ile eÄŸit
                if is_huge_dataset or is_massive_dataset:
                    x_train_df = pd.DataFrame(x_train, columns=feature_columns)
                    y_train_df = pd.Series(y_train)
                    best_model.fit(x_train_df, y_train_df)
                
                x_test_df = pd.DataFrame(x_test, columns=feature_columns)
                y_pred = best_model.predict(x_test_df)
            else:
                grid_search.fit(x_train_work, y_train_work)
                
                # En iyi modeli test et
                best_model = grid_search.best_estimator_
                
                # BÃ¼yÃ¼k dataset ise final modeli tam veri ile eÄŸit
                if is_huge_dataset or is_massive_dataset:
                    best_model.fit(x_train, y_train)
                
                y_pred = best_model.predict(x_test)
            
            # Performans metrikleri hesapla
            from sklearn.metrics import r2_score as r2_metric
            r2 = r2_metric(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            
            # Cross-validation skoru
            if model_name == 'lightgbm':
                cv_data_x = x_train_work_df if (is_huge_dataset or is_massive_dataset) else pd.DataFrame(x_train, columns=feature_columns)
                cv_data_y = y_train_work_df if (is_huge_dataset or is_massive_dataset) else pd.Series(y_train)
            else:
                cv_data_x = x_train_work if (is_huge_dataset or is_massive_dataset) else x_train
                cv_data_y = y_train_work if (is_huge_dataset or is_massive_dataset) else y_train
            cv_scores = cross_val_score(best_model, cv_data_x, cv_data_y, cv=cv_folds, scoring='r2')
            cv_mean = cv_scores.mean()
            cv_std = cv_scores.std()
            
            elapsed_time = time.time() - start_time
            
            return {
                'model_name': model_name,
                'model': best_model,
                'best_params': grid_search.best_params_,
                'r2_score': r2,
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'cv_mean': cv_mean,
                'cv_std': cv_std,
                'training_time': elapsed_time,
                'predictions': y_pred,
                'success': True
            }
            
        except Exception as e:
            return {
                'model_name': model_name,
                'error': str(e),
                'success': False
            }
    
    # Modelleri sÄ±ralÄ± olarak test et (erken durma ile)
    for model_name in model_order:
        if model_name not in models_config:
            continue
            
        if models_tested >= max_models_to_test:
            break
            
        config = models_config[model_name]
        try:
            
            # Timeout ile model eÄŸitimi
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(train_single_model, model_name, config)
                try:
                    result = future.result(timeout=max_time_per_model)
                    
                    if result['success']:
                        r2_score = result['r2_score']
                        
                        # Erken durma kontrolÃ¼
                        if r2_score < early_stop_threshold and models_tested > 2:
                            continue
                        
                        results.append(result)
                        models_tested += 1
                        
                        # Baseline gÃ¼ncelle
                        if r2_score > baseline_r2:
                            baseline_r2 = r2_score
                        
                        elapsed = result['training_time']
                        
                        # Ã‡ok iyi sonuÃ§ varsa erken bitir
                        if r2_score > 0.95 and models_tested >= 3:
                            break
                            
                    else:
                        pass
                        
                except TimeoutError:
                    future.cancel()
                    continue
                    
        except Exception as e:
            continue
    
    total_elapsed = time.time() - total_start_time
    
    # SonuÃ§larÄ± RÂ² skoruna gÃ¶re sÄ±rala (en yÃ¼ksekten en dÃ¼ÅŸÃ¼ÄŸe)
    results.sort(key=lambda x: x['r2_score'], reverse=True)
    
    if not results:
        raise Exception("HiÃ§bir model baÅŸarÄ±yla eÄŸitilemedi!")
    
    best_result = results[0]
    
    
    # TÃ¼m sonuÃ§larÄ± da dÃ¶ndÃ¼r (karÅŸÄ±laÅŸtÄ±rma iÃ§in)
    best_result['all_results'] = results
    
    return best_result

def get_model_comparison_summary(results):
    """
    TÃ¼m modellerin performans karÅŸÄ±laÅŸtÄ±rmasÄ±nÄ± Ã¶zetler.
    
    Args:
        results: find_best_model fonksiyonundan dÃ¶nen sonuÃ§
    
    Returns:
        str: KarÅŸÄ±laÅŸtÄ±rma Ã¶zeti
    """
    if 'all_results' not in results:
        return "KarÅŸÄ±laÅŸtÄ±rma verisi bulunamadÄ±."
    
    all_results = results['all_results']
    
    summary = "\n" + "="*60 + "\n"
    summary += "TUM MODELLERIN PERFORMANS KARSILASTIRMASI\n"
    summary += "="*60 + "\n"
    
    for i, result in enumerate(all_results, 1):
        summary += f"{i:2d}. {result['model_name'].upper():<15} "
        summary += f"R2: {result['r2_score']:.4f} "
        summary += f"RMSE: {result['rmse']:.4f} "
        summary += f"CV: {result['cv_mean']:.4f}+-{result['cv_std']:.4f}\n"
    
    summary += "="*60 + "\n"
    summary += f"KAZANAN: {results['model_name'].upper()}\n"
    summary += f"PERFORMANS ARTISI: "
    
    if len(all_results) > 1:
        second_best = all_results[1]['r2_score']
        improvement = ((results['r2_score'] - second_best) / second_best) * 100
        summary += f"{improvement:.2f}% daha iyi\n"
    else:
        summary += "Tek model test edildi\n"
    
    summary += "="*60
    
    return summary

def get_model_display_name(model_name):
    """Model adÄ±nÄ± kullanÄ±cÄ± dostu formata Ã§evirir."""
    display_names = {
        'linear_regression': 'Linear Regression',
        'ridge': 'Ridge Regression',
        'lasso': 'Lasso Regression',
        'elasticnet': 'ElasticNet Regression',
        'knn': 'KNN Regression',
        'svr': 'SVR (Support Vector Regression)',
        'decision_tree': 'Decision Tree',
        'random_forest': 'Random Forest',
        'xgboost': 'XGBoost',
        'lightgbm': 'LightGBM'
    }
    return display_names.get(model_name, model_name)
